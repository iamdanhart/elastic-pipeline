input {
    jdbc {
        jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/postgresql.jar"
        jdbc_driver_class => "org.postgresql.Driver"
        jdbc_connection_string => "jdbc:postgresql://postgres:5432/peopledb"
        jdbc_user => "postgres"
        jdbc_password => "P@ssw0rd"
        jdbc_fetch_size => 1000
        jdbc_paging_enabled => true
        #schedule => "* * * * *"
        statement => "SELECT id, xmlserialize(document xmlData AS TEXT) AS xmldata FROM people"
    }
}

filter {
    xml {
        source => "xmldata"
        store_xml => false

        xpath => [ 
            "/results/name/first/text()", "first_name", 
            "/results/name/last/text()", "last_name",
            "/results/location/street/number/text()", "street_number",
            "/results/location/street/name/text()", "street_name",
            "/results/dob/age/text()", "age",
            "/results/id/value/text()", "ssn"
        ]
    }
        
    mutate { 
        replace => [
            "first_name", "%{first_name}",
            "last_name", "%{last_name}",
            "street", "%{street_number} %{street_name}",
            "age", "%{age}",
            "ssn", "%{ssn}"
        ]

        remove_field => ["xmldata"]
        remove_field => ["street_number"]
        remove_field => ["street_name"]
    }

    mutate {
        convert => {"age" => "integer"}
    }
}

output {
    # stdout { }
    elasticsearch { 
        hosts => ["http://elasticsearch01:9200", "http://elasticsearch02:9200"] 
        index => "text_index"
        #document_id => "%{ssn}"     # uncomment this to add unique document IDs (duplicate data in the repo will result in only 3 records)
        ilm_enabled => false
    }
}